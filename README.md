# Neural Network from scratch

This repository contains code that will implement a neural network using only the python library numpy. Gradient descent, activation functions, and backpropagation are all implemented with just numpy.

## Usage

In order to run the example, paste the following command:

`python main.py`

## Implementation

The following are implemented:

- RELU and Sigmoid activation functions
- Forward propagation
- Backpropagation
- Cross Entropy Loss
- Derivatives of all the calculated functions

All of the code is batched, and will run with that in mind when the batchs are in the 0th dimension. All of the supporting functions and classes are inside the nn.py file.

## License

[MIT](<https://github.com/thksrc/nn_scratch/blob/master/LICENSE>)
